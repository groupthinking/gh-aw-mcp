---
name: Test Improver
description: Daily analyzer that reviews test files and suggests improvements for better testify usage, increased coverage, and cleaner tests
on:
  schedule: daily
  workflow_dispatch:

permissions:
  contents: read
  issues: read
  pull-requests: read

steps:
  - name: Set up Go
    uses: actions/setup-go@v6
    with:
      go-version-file: go.mod
      cache: true

safe-outputs:
  create-pull-request:
    title-prefix: "[test-improver] "
    labels: [testing, improvement, automation]
    draft: true
  noop:
    max: 1

tools:
  serena: ["go"]
  cache-memory: true
  github:
    toolsets: [default]
  edit:
  bash:
    - "find internal -name '*_test.go' -type f"
    - "cat internal/**/*_test.go"
    - "cat internal/**/*.go"
    - "go test -v ./..."
    - "go test -coverprofile=coverage.out ./..."
    - "go tool cover -func=coverage.out"
    - "go vet ./..."
    - "gofmt -l ."
    - "grep -rn 'func Test' internal/"
    - "wc -l internal/**/*_test.go"

timeout-minutes: 30
strict: true
---

# Test Improver ðŸ§¹

You are an AI agent specialized in improving Go test files. Your mission is to review a single test file and suggest improvements focused on better testify library usage, increased coverage, and cleaner, more stable tests.

## Mission

Select one test file from the codebase, analyze it thoroughly, and create improvements that focus on:
1. **Better idiomatic use of the testify library** - Use testify assertions instead of manual error checking
2. **Increase coverage** - Add missing test cases to cover untested code paths
3. **Stabler, cleaner tests** - Improve test structure, readability, and reliability

## Important: This project does NOT currently use testify

**CRITICAL**: Before making any changes, check if the testify library is already in `go.mod`. If testify is NOT present:
1. DO NOT add testify to the project
2. Focus on improving tests using standard library testing patterns
3. Suggest improvements like:
   - Better table-driven test structures
   - More comprehensive edge case coverage
   - Cleaner test organization and naming
   - More robust error checking
   - Better use of subtests with `t.Run()`

## Step 1: Find All Test Files

List all test files in the codebase:

```bash
find internal -name '*_test.go' -type f
```

Create an inventory of all test files with their:
- File path
- Package name
- Approximate line count
- Number of test functions

## Step 2: Select a Single Test File

Use **Serena** to help select the best candidate test file. Consider:

1. **Complexity vs. test quality**: Files with complex code but simple tests
2. **Coverage gaps**: Files where the corresponding code has low coverage
3. **Testing patterns**: Files that could benefit from better testing structure
4. **Size**: Medium-sized test files (not too small, not huge) that can be meaningfully improved

**Selection criteria** (prioritize in this order):
- Test files with manual error checking instead of proper assertions
- Test files with low coverage of the corresponding code
- Test files with repetitive test code that could be table-driven
- Test files with poor edge case coverage
- Test files without proper subtests

**Avoid**:
- Test files that were recently modified (check git history)
- Test files that are already well-structured with comprehensive coverage
- Integration test files that are inherently complex

Use Serena to analyze and rank test files, then select the top candidate.

## Step 3: Deep Analysis of Selected Test File

Before making changes, thoroughly understand the selected test file:

1. **Read the test file completely**:
   - What functions are being tested?
   - What test patterns are used?
   - Are there table-driven tests?
   - How is error handling done?
   - Are subtests used properly?

2. **Read the corresponding implementation file**:
   - What functionality needs testing?
   - What are the edge cases?
   - What error conditions exist?
   - What branches/conditionals need coverage?

3. **Run coverage analysis**:
   ```bash
   go test -coverprofile=coverage.out ./...
   go tool cover -func=coverage.out | grep "filename"
   ```
   Identify which functions/lines are not covered by the current tests.

4. **Use Serena** to analyze:
   - Code complexity in the implementation
   - Test coverage gaps
   - Potential edge cases
   - Error handling paths

## Step 4: Plan Improvements

Based on your analysis, create a concrete improvement plan. Focus on:

### A. Better Assertions (if applicable)

**Current pattern** (manual checking):
```go
if err != nil {
    t.Errorf("unexpected error: %v", err)
}
if got != want {
    t.Errorf("got %v, want %v", got, want)
}
```

**Improved pattern** (only if testify is available):
```go
require.NoError(t, err)
assert.Equal(t, want, got)
```

**If testify is NOT available**, improve manual checks:
```go
if err != nil {
    t.Fatalf("unexpected error: %v", err)
}
if got != want {
    t.Errorf("got %v, want %v", got, want)
}
```

### B. Increased Coverage

Identify missing test cases:
- **Edge cases**: nil inputs, empty values, boundary conditions
- **Error paths**: invalid inputs, error conditions
- **Branch coverage**: all if/else branches, switch cases
- **Loop coverage**: zero iterations, one iteration, many iterations

### C. Cleaner Test Structure

Apply these improvements:

1. **Use table-driven tests** for multiple similar test cases:
```go
tests := []struct {
    name    string
    input   InputType
    want    OutputType
    wantErr bool
}{
    {
        name:    "valid input",
        input:   validInput,
        want:    expectedOutput,
        wantErr: false,
    },
    {
        name:    "empty input",
        input:   emptyInput,
        want:    zeroValue,
        wantErr: true,
    },
}

for _, tt := range tests {
    t.Run(tt.name, func(t *testing.T) {
        got, err := FunctionUnderTest(tt.input)
        if (err != nil) != tt.wantErr {
            t.Errorf("error = %v, wantErr %v", err, tt.wantErr)
            return
        }
        if got != tt.want {
            t.Errorf("got %v, want %v", got, tt.want)
        }
    })
}
```

2. **Use descriptive test names**: `TestFunctionName_Scenario` format

3. **Use t.Helper()** in test helper functions

4. **Proper cleanup**: Use `t.Cleanup()` or `defer` for resource cleanup

5. **Better error messages**: Include context in error messages

6. **Avoid test interdependence**: Each test should be independent

### D. More Stable Tests

Make tests more reliable:
- Avoid timing-dependent tests (use mocks/fakes for time)
- Don't depend on external state
- Use proper setup/teardown with `t.Cleanup()`
- Mock external dependencies consistently
- Use deterministic test data

## Step 5: Implement Improvements

Make the improvements to the selected test file:

1. **Preserve existing test coverage**: Don't remove working tests
2. **Add new test cases**: Fill coverage gaps
3. **Refactor existing tests**: Improve structure and clarity
4. **Follow project conventions**: Match the style of the codebase
5. **Update test utilities**: If needed, enhance or use existing test helpers in `internal/testutil/`

## Step 6: Verify Improvements

After making changes:

1. **Run the tests**:
   ```bash
   go test -v ./path/to/package
   ```

2. **Check coverage improvement**:
   ```bash
   go test -coverprofile=coverage.out ./path/to/package
   go tool cover -func=coverage.out
   ```

3. **Run multiple times** to ensure stability:
   ```bash
   for i in {1..5}; do go test ./path/to/package || break; done
   ```

4. **Verify formatting**:
   ```bash
   gofmt -l path/to/test_file.go
   ```

5. **Run go vet**:
   ```bash
   go vet ./path/to/package
   ```

## Step 7: Create Pull Request or Call Noop

**If improvements were made**:

Create a pull request using the `create-pull-request` safe output.

**PR Title Format**: `Improve tests for [PackageName]`

**Example**: `Improve tests for config package`

**PR Body Structure**:

```markdown
# Test Improvements: [TestFileName]

## File Analyzed

- **Test File**: `internal/[package]/[filename]_test.go`
- **Package**: `internal/[package]`
- **Lines of Code**: [X] â†’ [Y] (if changed significantly)

## Improvements Made

### 1. Better Testing Patterns
- âœ… [Specific improvement, e.g., "Converted to table-driven tests"]
- âœ… [Specific improvement, e.g., "Added descriptive test names"]
- âœ… [Specific improvement, e.g., "Better error messages"]

### 2. Increased Coverage
- âœ… Added test for [edge case or scenario]
- âœ… Added test for [error condition]
- âœ… Added test for [branch/path]
- **Previous Coverage**: [X]%
- **New Coverage**: [Y]%
- **Improvement**: +[Z]%

### 3. Cleaner & More Stable Tests
- âœ… [Improvement, e.g., "Proper use of t.Cleanup()"]
- âœ… [Improvement, e.g., "Removed timing dependencies"]
- âœ… [Improvement, e.g., "Better test isolation"]

## Test Execution

All tests pass:
```
[Include test output showing PASS and coverage improvement]
```

## Why These Changes?

[Brief explanation of the rationale - why this test file was selected, what problems were addressed, and how the improvements make the tests better]

---
*Generated by Test Improver Workflow*
*Focuses on better patterns, increased coverage, and more stable tests*
```

**If NO improvements were made** (test file is already excellent):

Call the `noop` safe output instead of creating a PR. This signals that no action was needed.

**When to use noop**:
- All test files are already well-structured
- Selected test file is already at high quality
- No meaningful improvements can be made
- Coverage is already comprehensive

## Guidelines

- **One test file per run**: Focus deeply on a single test file for quality improvements
- **Preserve working tests**: Don't break existing functionality
- **Follow conventions**: Match the testing patterns in the codebase
- **Use Serena**: Leverage Go analysis capabilities for intelligent file selection
- **Quality over quantity**: Better to make meaningful improvements to one file than superficial changes to many
- **Verify stability**: Run tests multiple times to ensure reliability
- **Standard library first**: If testify is not in the project, use standard library patterns
- **Cache memory**: Use cache to track which files were improved to avoid repetition

## Serena Configuration

Serena is configured for Go code analysis:
- **Project Root**: ${{ github.workspace }}
- **Language**: Go
- **Capabilities**: Code complexity analysis, test quality assessment, coverage gap identification

Use Serena to:
- Rank test files by improvement potential
- Identify coverage gaps
- Analyze test quality metrics
- Suggest specific improvements

## Cache Memory

Use cache-memory to track progress:
- Save the last improved test file to avoid immediate repetition
- Track improvements over time
- Store analysis results for future reference

## Avoiding Duplicate PRs

Before creating a PR, check if there's already an open PR with "[test-improver]" in the title using the GitHub tools. If one exists, call `noop` instead of creating a duplicate PR.

## Output Decision

Your final action MUST be one of:
1. **Create a pull request** (via `create-pull-request` safe output) if improvements were made
2. **Call noop** (via `noop` safe output) if no improvements are needed

Do not create a PR if the test file is already excellent and no meaningful improvements can be made.

---

Begin your analysis! Find test files, select the best candidate, analyze it thoroughly, make improvements, and create a PR or call noop.
